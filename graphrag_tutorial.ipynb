{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0387db",
   "metadata": {},
   "source": [
    "# Graph RAG with OpenAI / Google Generative AI\n",
    "\n",
    "Graph retrieval augmented generation (Graph RAG) is emerging as a powerful technique for generative AI applications to use domain-specific knowledge and relevant information. Graph RAG is an alternative to vector search methods that use a vector database. Knowledge graphs are knowledge systems where graph databases such as Neo4j or Amazon Neptune can represent structured data. In a knowledge graph, the relationships between data points, called edges, are as meaningful as the connections between data points, called vertices or sometimes nodes. A knowledge graph makes it easy to traverse a network and process complex queries about connected data. Knowledge graphs are especially well suited for use cases involving chatbots, identity resolution, network analysis, recommendation engines, customer 360 and fraud detection.\n",
    "\n",
    "A Graph RAG approach leverages the structured nature of graph databases to give greater depth and context of retrieved information about networks or complex relationships. When a graph database is paired with a large language model (LLM), a developer can automate significant parts of the graph creation process from unstructured data like text. An LLM can process text data and identify entities, understand their relationships and represent them in a graph structure.\n",
    "\n",
    "There are many ways to create a Graph RAG application, for instance Microsoft's GraphRAG, or pairing GPT4 with LlamaIndex. **For this tutorial you'll use Memgraph, an open source graph database solution to create a rag system by using a Large Language Model from OpenAI or Google Generative AI.** Memgraph uses Cypher, a declarative query language. It shares some similarities with SQL but focuses on nodes and relationships rather than tables and rows. You'll have the LLM both create and populate your graph database from unstructured text and query information in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09d942",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "**This tutorial uses OpenAI or Google Generative AI. You will need to obtain an API key from the provider of your choice.**\n",
    "\n",
    "a. Obtain an [OpenAI API Key](https://platform.openai.com/api-keys).\n",
    "\n",
    "b. Or, obtain a [Google API Key](https://aistudio.google.com/app/apikey).\n",
    "\n",
    "## Step 2\n",
    "\n",
    "Now, you'll need to install Docker from [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)\n",
    "\n",
    "Once you've installed Docker, install Memgraph using their Docker container. On OSX or Linux, you can use this command in a terminal:\n",
    "\n",
    "    curl https://install.memgraph.com | sh\n",
    "\n",
    "On a Windows computer use:\n",
    "\n",
    "    iwr https://windows.memgraph.com | iex\n",
    "\n",
    "Follow the installation steps to get the Memgraph engine and Memgraph lab up and running.\n",
    "\n",
    "## Step 3\n",
    "\n",
    "On your computer, create a fresh virtualenv for this project:\n",
    "\n",
    "    python -m venv graphrag-env\n",
    "    source graphrag-env/bin/activate  # On Windows: graphrag-env\\Scripts\\activate\n",
    "\n",
    "In the Python environment for your notebook, install the required Python libraries:\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "Now you're ready to connect to Memgraph.\n",
    "\n",
    "## Step 4 - MemgraphæŽ¥ç¶šè¨­å®š\n",
    "\n",
    "### ðŸ” èªè¨¼ã«ã¤ã„ã¦\n",
    "\n",
    "**é‡è¦**: ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€Memgraphã®èªè¨¼ã¯ç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ¼ãƒ ãƒ»ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯ä¸è¦**ã§ã™\n",
    "- ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™»éŒ²ã‚„ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“  \n",
    "- ç©ºã®èªè¨¼æƒ…å ±ï¼ˆç©ºæ–‡å­—åˆ—ï¼‰ã§ãã®ã¾ã¾æŽ¥ç¶šã§ãã¾ã™\n",
    "\n",
    "ã“ã‚Œã¯ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒç”¨ã®è¨­å®šã§ã€æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã‚ãªã„å ´åˆã«é©ã—ã¦ã„ã¾ã™ã€‚æœ¬æ ¼çš„ãªé‹ç”¨ç’°å¢ƒã§ã¯èªè¨¼ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã‚’æŽ¨å¥¨ã—ã¾ã™ã€‚\n",
    "\n",
    "If you've configured Memgraph to use a username and password, set them here, otherwise you can use the defaults of having neither. It's not good practice for a production database but for a local development environment that doesn't store sensitive data, it's not an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "from langchain_community.chains.graph_qa.memgraph import MemgraphQAChain\n",
    "from langchain_community.graphs import MemgraphGraph\n",
    " \n",
    "url = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\")\n",
    "username = os.environ.get(\"MEMGRAPH_USERNAME\", \"\")\n",
    "password = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")\n",
    "\n",
    "# initialize memgraph connection\n",
    "graph = MemgraphGraph(\n",
    "    url=url, username=username, password=password, refresh_schema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b1b7e",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Now create a sample string that describes a dataset of relationships that you can use to test the graph generating capabilities of your LLM system. You can use more complex data sources but this simple example helps us demonstrate the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_text = \"\"\"\n",
    "John's title is Director of the Digital Marketing Group. John works with Jane whose title is Chief Marketing Officer. Jane works in the Executive Group. Jane works with Sharon whose title is the Director of Client Outreach. Sharon works in the Sales Group. John belongs to the Digital Marketing Group.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650f21e",
   "metadata": {},
   "source": [
    "Enter the API key you created in the first step. Choose either OpenAI or Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Uncomment the API key you want to use\n",
    "\n",
    "# For OpenAI\n",
    "api_key = getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# For Google Generative AI\n",
    "# api_key = getpass(\"Enter your Google API Key: \")\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc32dab",
   "metadata": {},
   "source": [
    "Now configure an LLM instance to generate text. The temperature should be set to 0 to encourage the model to generate factual details based on the input text without hallucinating entities or relationships that aren't present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LLM class you want to use\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Select and initialize the LLM for graph generation.\n",
    "# We use a temperature of 0 to get deterministic and factual results.\n",
    "\n",
    "# Using OpenAI's GPT-4o-mini (supports structured output)\n",
    "llm_for_graph_generation = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Using Google's Gemini Pro\n",
    "# llm_for_graph_generation = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdeb971",
   "metadata": {},
   "source": [
    "The `LLMGraphTransformer` allows you to set what kinds of nodes and relationships you'd like the LLM to generate. In your case, the text describes employees at a company, the groups they work in and their job titles. Restricting the LLM to just those entities makes it more likely that you'll get a good representation of the knowledge in a graph.\n",
    "\n",
    "The call to `convert_to_graph_documents` has the LLMGraphTransformer create a knowledge graph from the text. This step generates the correct Cypher syntax to insert the information into the graph database to represent the relevant context and relevant entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm_for_graph_generation, \n",
    "    allowed_nodes=[\"Person\", \"Title\", \"Group\"],\n",
    "    allowed_relationships=[\"TITLE\", \"COLLABORATES\", \"GROUP\"]\n",
    ")\n",
    "documents = [Document(page_content=graph_text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a35032",
   "metadata": {},
   "source": [
    "Now clear any old data out of the Memgraph database and insert the new nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the database is empty\n",
    "# Note: The original tutorial had specific storage mode commands which might not be necessary \n",
    "# for a simple setup. We'll proceed by clearing the graph directly.\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    " \n",
    "# create knowledge graph\n",
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0064454",
   "metadata": {},
   "source": [
    "The generated Cypher syntax is stored in the `graph_documents` objects. You can inspect it simply by printing it as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145caaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{graph_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cf6b6",
   "metadata": {},
   "source": [
    "The schema and data types created by the Cypher can be seen in the graphs `get_schema` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Let's see what data was actually created\n",
    "print(\"ðŸ” Checking actual data in the graph...\")\n",
    "\n",
    "try:\n",
    "    # Check Person nodes\n",
    "    person_nodes = graph.query(\"MATCH (p:Person) RETURN p.id as person_id\")\n",
    "    print(f\"ðŸ‘¥ Person nodes: {person_nodes}\")\n",
    "    \n",
    "    # Check Title nodes  \n",
    "    title_nodes = graph.query(\"MATCH (t:Title) RETURN t.id as title_id\")\n",
    "    print(f\"ðŸ’¼ Title nodes: {title_nodes}\")\n",
    "    \n",
    "    # Check Group nodes\n",
    "    group_nodes = graph.query(\"MATCH (g:Group) RETURN g.id as group_id\") \n",
    "    print(f\"ðŸ¢ Group nodes: {group_nodes}\")\n",
    "    \n",
    "    # Check relationships\n",
    "    relationships = graph.query(\"MATCH (n)-[r]->(m) RETURN type(r), n.id, m.id LIMIT 10\")\n",
    "    print(f\"ðŸ”— Relationships: {relationships}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Debug queries failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5895369",
   "metadata": {},
   "source": [
    "You can also see the graph structure in the Memgraph labs viewer by connecting to your local instance and running the query `MATCH (n)-[r]->(m) RETURN n,r,m`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb9a62",
   "metadata": {},
   "source": [
    "The LLM has done a reasonable job of creating the correct nodes and relationships. Now it's time to query the knowledge graph.\n",
    "\n",
    "## Step 6\n",
    "\n",
    "Prompting the LLM correctly requires some prompt engineering. LangChain provides a FewShotPromptTemplate that can be used to give examples to the LLM in the prompt to ensure that it writes correct and succinct Cypher syntax. The following code gives several examples of questions and queries that the LLM should use. It also shows constraining the output of the model to only the query. An overly chatty LLM might add in extra information that would lead to invalid Cypher queries, so the prompt template asks the model to output only the query itself.\n",
    "\n",
    "Adding an instructive prefix also helps to constrain the model behavior and makes it more likely that the LLM will output correct Cypher syntax. **Note that the Llama-3 specific tokens have been removed from the examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce765a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# The examples are updated to match actual data structure\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What is John's title?\",\n",
    "        \"query\": \"MATCH (p:Person {id: 'John'})-[:TITLE]->(t:Title) RETURN t.id\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who does John collaborate with?\",\n",
    "        \"query\": \"MATCH (p:Person {id: 'John'})-[:COLLABORATES]->(c:Person) RETURN c.id\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What group is Jane in?\",\n",
    "        \"query\": \"MATCH (p:Person {id: 'Jane'})-[:GROUP]->(g:Group) RETURN g.id\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a simpler, safer prompt template approach\n",
    "# Build the examples string manually to avoid template conflicts\n",
    "examples_text = \"\"\n",
    "for example in examples:\n",
    "    examples_text += f\"User input: {example['question']}\\nCypher query: {example['query']}\\n\\n\"\n",
    "\n",
    "# Create a simple prompt template that avoids conflicts with schema content\n",
    "cypher_prompt_template = \"\"\"You are a Cypher query expert. Given a schema and a question, you must create a syntactically correct Cypher query to answer the question.\n",
    "You must respond with ONLY the query, with no other text, explanation, or context.\n",
    "You must use the provided node and relationship labels and property names from the schema.\n",
    "\n",
    "Here is the schema:\n",
    "{{schema}}\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "{examples_text}User input: {{question}}\n",
    "Cypher query: \"\"\"\n",
    "\n",
    "cypher_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    "    template=cypher_prompt_template.format(examples_text=examples_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea06bd",
   "metadata": {},
   "source": [
    "Next, you'll create a prompt to control how the LLM answers the question with the information returned from Memgraph. We'll give the LLM several examples and instructions on how to respond once it has context information back from the graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af28a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\n",
    "You are a helpful assistant that answers user questions based on the context provided.\n",
    "If the context is empty, say you don't know the answer.\n",
    "Use only the information provided in the context to answer the question.\n",
    "Your answer should be concise and directly answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(qa_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f64d60",
   "metadata": {},
   "source": [
    "Now it's time to create the question answering chain. The `MemgraphQAChain` allows you to set which LLM you'd like to use and the graph schema to be used. We will reuse the same LLM instance or create a new one for this chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324644a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and initialize the LLM for the QA part of the chain.\n",
    "# It can be the same as the one used for graph generation.\n",
    "\n",
    "# Using OpenAI's GPT-4o-mini (supports structured output)\n",
    "llm_for_qa = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Using Google's Gemini Pro\n",
    "# llm_for_qa = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0, convert_system_message_to_human=True)\n",
    "\n",
    "# Note: allow_dangerous_requests=True is required for security acknowledgment\n",
    "# This chain can potentially execute dangerous Cypher queries\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    llm = llm_for_qa,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt=qa_prompt,\n",
    "    allow_dangerous_requests=True  # Required for LangChain security\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2315f",
   "metadata": {},
   "source": [
    "Now you can invoke the chain with a natural language question (note that your responses might be slightly different because LLMs are not purely deterministic, but should be consistent with a temperature of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ee439",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is Johns title?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6de90",
   "metadata": {},
   "source": [
    "In the next question, ask the chain a slightly more complex question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Who does John collaborate with?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dffb74",
   "metadata": {},
   "source": [
    "You can ask the Memgraph chain about Group relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What group is Jane in?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6bb3e",
   "metadata": {},
   "source": [
    "The chain correctly identifies the relationship.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this tutorial, you built a Graph RAG application using Memgraph and a large language model from OpenAI or Google Generative AI to generate the graph data structures and query them. Using an LLM, you extracted node and edge information from natural language source text and generated Cypher query syntax to populate a graph database. You then used the same or another LLM to turn natural language questions about that source text into Cypher queries that extracted information from the graph database. Using prompt engineering, the LLM turned the results from the Memgraph database into natural language responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
